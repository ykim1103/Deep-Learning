{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt   # 그림그리기 위한 import\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR gate\n",
    "-참고: https://docs.google.com/presentation/d/1KHpjyziDm0Wle-OI-6TZhWM2Oj7YiypXuZOZ1SJW8ds/edit#slide=id.g1d14145db6_0_5\n",
    "\n",
    "\n",
    "### OR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],[1, 0, 0],[1, 1, 0],[1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[1],[1],[1],[1],[1]], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= tf.placeholder(tf.float32,shape=[None,3])\n",
    "y= tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "W= tf.Variable(tf.random_normal([3,1]),dtype=tf.float32)\n",
    "b= tf.Variable(tf.random_normal([1]),dtype=tf.float32)\n",
    "\n",
    "hypot = tf.sigmoid(tf.matmul(X,W)+b)\n",
    "cost = -tf.reduce_mean(y*tf.log(hypot)+(1-y)*tf.log(1-hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(1000):\n",
    "    sess.run(train, feed_dict={X:x_data,y:y_data})\n",
    "    \n",
    "    \n",
    "h,p,a = sess.run([hypot,pred,accuracy],feed_dict={X:x_data,y:y_data})    \n",
    "\n",
    "print('가설: ',h, '\\n예측: ',p, '\\n정확도: ',a)\n",
    "\n",
    "\n",
    "sess.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AND gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],[1, 0, 0],[1, 1, 0],[1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0],[0],[0],[0],[0],[0],[0],[1]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= tf.placeholder(tf.float32,shape=[None,3])\n",
    "y= tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "W= tf.Variable(tf.random_normal([3,1]),dtype=tf.float32)\n",
    "b= tf.Variable(tf.random_normal([1]),dtype=tf.float32)\n",
    "\n",
    "hypot = tf.sigmoid(tf.matmul(X,W)+b)\n",
    "cost = -tf.reduce_mean(y*tf.log(hypot)+(1-y)*tf.log(1-hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(1000):\n",
    "    sess.run(train, feed_dict={X:x_data,y:y_data})\n",
    "    \n",
    "    \n",
    "h,p,a = sess.run([hypot,pred,accuracy],feed_dict={X:x_data,y:y_data})    \n",
    "\n",
    "print('가설: ',h, '\\n예측: ',p, '\\n정확도: ',a)\n",
    "\n",
    "\n",
    "sess.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],[1, 0, 0],[1, 1, 0],[1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[1],[1],[1],[1],[0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= tf.placeholder(tf.float32,shape=[None,3])\n",
    "y= tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "W= tf.Variable(tf.random_normal([3,1]),dtype=tf.float32)\n",
    "b= tf.Variable(tf.random_normal([1]),dtype=tf.float32)\n",
    "\n",
    "hypot = tf.sigmoid(tf.matmul(X,W)+b)\n",
    "cost = -tf.reduce_mean(y*tf.log(hypot)+(1-y)*tf.log(1-hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(1000):\n",
    "    sess.run(train, feed_dict={X:x_data,y:y_data})\n",
    "    \n",
    "    \n",
    "h,p,a = sess.run([hypot,pred,accuracy],feed_dict={X:x_data,y:y_data})    \n",
    "\n",
    "print('가설: ',h, '\\n예측: ',p, '\\n정확도: ',a)\n",
    "\n",
    "\n",
    "sess.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm,metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],[1, 0, 0],[1, 1, 0],[1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[1],[1],[1],[1],[0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=svm.SVC(C=100)\n",
    "clf.fit(x_data,y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=[[1,1,1],[0,0,0],[0,0,1],[0,1,0]]\n",
    "\n",
    "test_label=[0,0,1,1]\n",
    "\n",
    "result=clf.predict(test)\n",
    "print(result)\n",
    "\n",
    "score=metrics.accuracy_score(test_label,result)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 딥러닝을 이용한 XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],[1, 0, 0],[1, 1, 0],[1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[1],[1],[1],[1],[0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= tf.placeholder(tf.float32,shape=[None,3])\n",
    "y= tf.placeholder(tf.float32,shape=[None,1])    # 최종출력\n",
    "\n",
    "\n",
    "# 첫번째 히든계층\n",
    "W1= tf.Variable(tf.random_normal([3,10]),dtype=tf.float32)     # 출력갯수가 10개\n",
    "b1= tf.Variable(tf.random_normal([10]),dtype=tf.float32)\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "\n",
    "\n",
    "W2= tf.Variable(tf.random_normal([10,1]),dtype=tf.float32)      # W1에서 출력한 10개를 입력받음. 최종출력 1개\n",
    "b2= tf.Variable(tf.random_normal([1]),dtype=tf.float32)\n",
    "hypot = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hypot)+(1-y)*tf.log(1-hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(10000):\n",
    "    sess.run(train, feed_dict={X:x_data,y:y_data})\n",
    "    \n",
    "    \n",
    "h,p,a = sess.run([hypot,pred,accuracy],feed_dict={X:x_data,y:y_data})    \n",
    "\n",
    "print('가설: ',h, '\\n예측: ',p, '\\n정확도: ',a)\n",
    "\n",
    "\n",
    "sess.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep & Wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Deep : 6개의 히든레이어 추가\n",
    "# # 각 계층의 입출력 갯수는 50개\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],[1, 0, 0],[1, 1, 0],[1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[1],[1],[1],[1],[0]], dtype=np.float32)\n",
    "\n",
    "\n",
    "X= tf.placeholder(tf.float32,shape=[None,3])\n",
    "y= tf.placeholder(tf.float32,shape=[None,1])    # 최종출력\n",
    "\n",
    "\n",
    "# 첫번째 히든계층\n",
    "W1= tf.Variable(tf.random_normal([3,50]),dtype=tf.float32)     \n",
    "b1= tf.Variable(tf.random_normal([50]),dtype=tf.float32)\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "\n",
    "\n",
    "W2= tf.Variable(tf.random_normal([50,50]),dtype=tf.float32)      \n",
    "b2= tf.Variable(tf.random_normal([50]),dtype=tf.float32)\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "\n",
    "W3= tf.Variable(tf.random_normal([50,50]),dtype=tf.float32)     \n",
    "b3= tf.Variable(tf.random_normal([50]),dtype=tf.float32)\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2,W3)+b3)\n",
    "\n",
    "\n",
    "W4= tf.Variable(tf.random_normal([50,50]),dtype=tf.float32)     \n",
    "b4= tf.Variable(tf.random_normal([50]),dtype=tf.float32)\n",
    "layer4 = tf.sigmoid(tf.matmul(layer3,W4)+b4)\n",
    "\n",
    "\n",
    "W5= tf.Variable(tf.random_normal([50,50]),dtype=tf.float32)     \n",
    "b5= tf.Variable(tf.random_normal([50]),dtype=tf.float32)\n",
    "layer5 = tf.sigmoid(tf.matmul(layer4,W5)+b5)\n",
    "\n",
    "W6= tf.Variable(tf.random_normal([50,50]),dtype=tf.float32)     \n",
    "b6= tf.Variable(tf.random_normal([50]),dtype=tf.float32)\n",
    "layer6 = tf.sigmoid(tf.matmul(layer5,W6)+b6)\n",
    "\n",
    "\n",
    "W7= tf.Variable(tf.random_normal([50,1]),dtype=tf.float32)     \n",
    "b7= tf.Variable(tf.random_normal([1]),dtype=tf.float32)\n",
    "hypot = tf.sigmoid(tf.matmul(layer6,W7)+b7)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hypot)+(1-y)*tf.log(1-hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(10000):\n",
    "    sess.run(train, feed_dict={X:x_data,y:y_data})\n",
    "    \n",
    "    \n",
    "h,p,a = sess.run([hypot,pred,accuracy],feed_dict={X:x_data,y:y_data})    \n",
    "\n",
    "print('가설: ',h, '\\n예측: ',p, '\\n정확도: ',a)\n",
    "\n",
    "\n",
    "sess.close()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TLU 1: n-> 입력값을 가장 처음으로 전달받음\n",
    "W1 = tf.Variable(tf.random_normal([3, 50]), dtype=tf.float32)\n",
    "b1 = tf.Variable(tf.random_normal([50]), dtype=tf.float32)\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# TLU 2\n",
    "W2 = tf.Variable(tf.random_normal([50, 50]), dtype=tf.float32)\n",
    "b2 = tf.Variable(tf.random_normal([50]), dtype=tf.float32)\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "# TLU 3\n",
    "W3 = tf.Variable(tf.random_normal([50, 50]), dtype=tf.float32)\n",
    "b3 = tf.Variable(tf.random_normal([50]), dtype=tf.float32)\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "# TLU 4\n",
    "W4 = tf.Variable(tf.random_normal([50, 50]), dtype=tf.float32)\n",
    "b4 = tf.Variable(tf.random_normal([50]), dtype=tf.float32)\n",
    "layer4 = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "# TLU 5\n",
    "W5 = tf.Variable(tf.random_normal([50, 50]), dtype=tf.float32)\n",
    "b5 = tf.Variable(tf.random_normal([50]), dtype=tf.float32)\n",
    "layer5 = tf.sigmoid(tf.matmul(layer4, W5) + b5)\n",
    "\n",
    "# TLU 6\n",
    "W6 = tf.Variable(tf.random_normal([50, 1]), dtype=tf.float32)\n",
    "b6 = tf.Variable(tf.random_normal([1]), dtype=tf.float32)\n",
    "hypot = tf.sigmoid(tf.matmul(layer5, W6) + b6)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hypot) + (1-y) * tf.log(1-hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,y), dtype=tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "for epoch in range(10000): \n",
    "    sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "    \n",
    "h, p, a = sess.run([hypot, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "\n",
    "\n",
    "print(\"가설 :\", h, \"\\n예측:\", p, \"\\n정확도:\", a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설:  [[0.0173741 ]\n",
      " [0.9965475 ]\n",
      " [0.99644583]\n",
      " [0.9851097 ]\n",
      " [0.99581903]\n",
      " [0.985903  ]\n",
      " [0.9878019 ]\n",
      " [0.03531206]] \n",
      "예측:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "정확도:  1.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()       # 재실행하기위해 초기화 해주는 함수\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],[1, 0, 0],[1, 1, 0],[1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[1],[1],[1],[1],[0]], dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "X= tf.placeholder(tf.float32,shape=[None,3])\n",
    "y= tf.placeholder(tf.float32,shape=[None,1])    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# with tf.name_scope('layer1'):     # with을 이용해서 그룹으로 묶어준다.  \n",
    "#     W1= tf.Variable(tf.random_normal([3,10]),dtype=tf.float32)     \n",
    "#     b1= tf.Variable(tf.random_normal([10]),dtype=tf.float32)\n",
    "#     layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "\n",
    "W1= tf.Variable(tf.random_normal([3,10]),dtype=tf.float32)     \n",
    "b1= tf.Variable(tf.random_normal([10]),dtype=tf.float32)\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "\n",
    "\n",
    "W2= tf.Variable(tf.random_normal([10,1]),dtype=tf.float32)      \n",
    "b2= tf.Variable(tf.random_normal([1]),dtype=tf.float32)\n",
    "hypot = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "\n",
    "tf.summary.histogram('weight2',W2)\n",
    "\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hypot)+(1-y)*tf.log(1-hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "tf.summary.scalar('cost',cost)\n",
    "\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('log_dir2/alpha01')\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "\n",
    "for step in range(10000):\n",
    "    _,summary =sess.run([train, merged_summary],feed_dict={X:x_data,y:y_data})\n",
    "    writer.add_summary(summary,global_step=step)\n",
    "    \n",
    "h,p,a = sess.run([hypot,pred,accuracy],feed_dict={X:x_data,y:y_data})    \n",
    "\n",
    "print('가설: ',h, '\\n예측: ',p, '\\n정확도: ',a)\n",
    "\n",
    "\n",
    "sess.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmd창에서 실행\n",
    "\n",
    "# log_dir2폴더 전까지 접근 후\n",
    "# tensorboard --logdir=./log_dir2/alpha01 \n",
    "# http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Fetch argument <tf.Operation 'GradientDescent' type=NoOp> cannot be interpreted as a Tensor. (Operation name: \"GradientDescent\"\nop: \"NoOp\"\ninput: \"^GradientDescent/update_layer1/Variable/ApplyGradientDescent\"\ninput: \"^GradientDescent/update_layer1/Variable_1/ApplyGradientDescent\"\ninput: \"^GradientDescent/update_layer2/Variable/ApplyGradientDescent\"\ninput: \"^GradientDescent/update_layer2/Variable_1/ApplyGradientDescent\"\n is not an element of this graph.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    304\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[1;32m--> 305\u001b[1;33m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[0;32m    306\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3606\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3607\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3690\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3691\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Operation %s is not an element of this graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3692\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Operation name: \"GradientDescent\"\nop: \"NoOp\"\ninput: \"^GradientDescent/update_layer1/Variable/ApplyGradientDescent\"\ninput: \"^GradientDescent/update_layer1/Variable_1/ApplyGradientDescent\"\ninput: \"^GradientDescent/update_layer2/Variable/ApplyGradientDescent\"\ninput: \"^GradientDescent/update_layer2/Variable_1/ApplyGradientDescent\"\n is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e80b6c96b97b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerged_summary\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_data\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1163\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1164\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[1;32m-> 1165\u001b[1;33m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[0;32m   1166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    472\u001b[0m     \"\"\"\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m       \u001b[1;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections_abc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \"\"\"\n\u001b[0;32m    374\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \"\"\"\n\u001b[0;32m    374\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m     \u001b[1;31m# Did not find anything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' %\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    310\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n\u001b[1;32m--> 312\u001b[1;33m                          'Tensor. (%s)' % (fetch, str(e)))\n\u001b[0m\u001b[0;32m    313\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[1;31mValueError\u001b[0m: Fetch argument <tf.Operation 'GradientDescent' type=NoOp> cannot be interpreted as a Tensor. (Operation name: \"GradientDescent\"\nop: \"NoOp\"\ninput: \"^GradientDescent/update_layer1/Variable/ApplyGradientDescent\"\ninput: \"^GradientDescent/update_layer1/Variable_1/ApplyGradientDescent\"\ninput: \"^GradientDescent/update_layer2/Variable/ApplyGradientDescent\"\ninput: \"^GradientDescent/update_layer2/Variable_1/ApplyGradientDescent\"\n is not an element of this graph.)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tf.reset_default_graph()       # 재실행하기위해 초기화 해주는 함수\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],[1, 0, 0],[1, 1, 0],[1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[1],[1],[1],[1],[0]], dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "X= tf.placeholder(tf.float32,shape=[None,3])\n",
    "y= tf.placeholder(tf.float32,shape=[None,1])    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope('layer1'):     # with을 이용해서 그룹으로 묶어준다.  \n",
    "    W1= tf.Variable(tf.random_normal([3,10]),dtype=tf.float32)     \n",
    "    b1= tf.Variable(tf.random_normal([10]),dtype=tf.float32)\n",
    "    layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "    tf.summary.histogram('weight1',W1)\n",
    "    tf.summary.histogram('bias1',b1)\n",
    "    tf.summary.histogram('layer1',layer1)\n",
    "    \n",
    "    \n",
    "    \n",
    "with tf.name_scope('layer2'):    \n",
    "    W2= tf.Variable(tf.random_normal([10,1]),dtype=tf.float32)      \n",
    "    b2= tf.Variable(tf.random_normal([1]),dtype=tf.float32)\n",
    "    hypot = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "    tf.summary.histogram('weight2',W2)\n",
    "    tf.summary.histogram('bias2',b2)\n",
    "    tf.summary.histogram('hypot',hypot)\n",
    "\n",
    "\n",
    "with tf.name_scope('cost'):\n",
    "    cost = -tf.reduce_mean(y*tf.log(hypot)+(1-y)*tf.log(1-hypot))\n",
    "    tf.summary.scalar('cost',cost )\n",
    "\n",
    "\n",
    "\n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('log_dir2/alpha01')\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "\n",
    "for step in range(10000):\n",
    "    _,summary =sess.run([train, merged_summary],feed_dict={X:x_data,y:y_data})\n",
    "    writer.add_summary(summary,global_step=step)\n",
    "    \n",
    "h,p,a = sess.run([hypot,pred,accuracy],feed_dict={X:x_data,y:y_data})    \n",
    "\n",
    "print('가설: ',h, '\\n예측: ',p, '\\n정확도: ',a)\n",
    "\n",
    "\n",
    "sess.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설:  [[0.4102134 ]\n",
      " [0.91261154]\n",
      " [0.8330385 ]\n",
      " [0.74872714]\n",
      " [0.8073429 ]\n",
      " [0.8703625 ]\n",
      " [0.89745104]\n",
      " [0.52475774]] \n",
      "예측:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "정확도:  0.875\n"
     ]
    }
   ],
   "source": [
    "#learning_rate=0.01일 때 확인\n",
    "\n",
    "tf.reset_default_graph()       # 재실행하기위해 초기화 해주는 함수\n",
    "\n",
    "\n",
    "x_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],[1, 0, 0],[1, 1, 0],[1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[1],[1],[1],[1],[0]], dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "X= tf.placeholder(tf.float32,shape=[None,3])\n",
    "y= tf.placeholder(tf.float32,shape=[None,1])    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope('layer1'):     # with을 이용해서 그룹으로 묶어준다.  \n",
    "    W1= tf.Variable(tf.random_normal([3,10]),dtype=tf.float32)     \n",
    "    b1= tf.Variable(tf.random_normal([10]),dtype=tf.float32)\n",
    "    layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "    tf.summary.histogram('weight1',W1)\n",
    "    tf.summary.histogram('bias1',b1)\n",
    "    tf.summary.histogram('layer1',layer1)\n",
    "    \n",
    "    \n",
    "    \n",
    "with tf.name_scope('layer2'):    \n",
    "    W2= tf.Variable(tf.random_normal([10,1]),dtype=tf.float32)      \n",
    "    b2= tf.Variable(tf.random_normal([1]),dtype=tf.float32)\n",
    "    hypot = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "    tf.summary.histogram('weight2',W2)\n",
    "    tf.summary.histogram('bias2',b2)\n",
    "    tf.summary.histogram('hypot',hypot)\n",
    "\n",
    "\n",
    "with tf.name_scope('cost'):\n",
    "    cost = -tf.reduce_mean(y*tf.log(hypot)+(1-y)*tf.log(1-hypot))\n",
    "    tf.summary.scalar('cost',cost )\n",
    "\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "    \n",
    "pred = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('log_dir2/alpha01')\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "\n",
    "for step in range(10000):\n",
    "    _,summary =sess.run([train, merged_summary],feed_dict={X:x_data,y:y_data})\n",
    "    writer.add_summary(summary,global_step=step)\n",
    "    \n",
    "h,p,a = sess.run([hypot,pred,accuracy],feed_dict={X:x_data,y:y_data})    \n",
    "\n",
    "print('가설: ',h, '\\n예측: ',p, '\\n정확도: ',a)\n",
    "\n",
    "\n",
    "sess.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir=./log_dir2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-180cef80452b>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)\n",
    "mnist = input_data.read_data_sets('data/MNIST_data',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 2.8511620274457097        accuracy: 0.725\n",
      "epoch: 2     cost: 1.1251461763815447        accuracy: 0.78\n",
      "epoch: 3     cost: 0.8907791806351046        accuracy: 0.815\n",
      "epoch: 4     cost: 0.7743417363817039        accuracy: 0.85\n",
      "epoch: 5     cost: 0.7010737030072647        accuracy: 0.8\n",
      "epoch: 6     cost: 0.649825843139128        accuracy: 0.9\n",
      "epoch: 7     cost: 0.6103813851963387        accuracy: 0.865\n",
      "epoch: 8     cost: 0.5799001266739585        accuracy: 0.86\n",
      "epoch: 9     cost: 0.5542897810719231        accuracy: 0.88\n",
      "epoch: 10     cost: 0.5329793524742126        accuracy: 0.89\n",
      "epoch: 11     cost: 0.5146557521820065        accuracy: 0.85\n",
      "epoch: 12     cost: 0.4984663052450528        accuracy: 0.87\n",
      "epoch: 13     cost: 0.48534085934812377        accuracy: 0.915\n",
      "epoch: 14     cost: 0.47311392442746575        accuracy: 0.89\n",
      "epoch: 15     cost: 0.46182324068112834        accuracy: 0.91\n",
      "epoch: 16     cost: 0.45209310634569694        accuracy: 0.915\n",
      "epoch: 17     cost: 0.44294299001043463        accuracy: 0.93\n",
      "epoch: 18     cost: 0.43477278844876704        accuracy: 0.895\n",
      "epoch: 19     cost: 0.42720184613357864        accuracy: 0.88\n",
      "epoch: 20     cost: 0.4203329159996727        accuracy: 0.885\n",
      "epoch: 21     cost: 0.41382963581518695        accuracy: 0.89\n",
      "epoch: 22     cost: 0.4075854258103805        accuracy: 0.91\n",
      "epoch: 23     cost: 0.4019695848226548        accuracy: 0.88\n",
      "epoch: 24     cost: 0.3965175001729618        accuracy: 0.905\n",
      "epoch: 25     cost: 0.3918375598842447        accuracy: 0.91\n",
      "epoch: 26     cost: 0.3874423116445544        accuracy: 0.885\n",
      "epoch: 27     cost: 0.3830055846951221        accuracy: 0.915\n",
      "epoch: 28     cost: 0.3789297766577117        accuracy: 0.945\n",
      "epoch: 29     cost: 0.3752858047593722        accuracy: 0.895\n",
      "epoch: 30     cost: 0.3712532819942995        accuracy: 0.88\n",
      "훈련 종료\n",
      "정확도 :  0.901\n"
     ]
    }
   ],
   "source": [
    "# 성능 90%\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([28*28, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.2).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c, acc = sess.run([train, cost,accuracy], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost, \"       accuracy:\",acc)\n",
    "        \n",
    "print(\"훈련 종료\")\n",
    "\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 2.762731569897045        accuracy: 0.545\n",
      "epoch: 2     cost: 1.3317632180994206        accuracy: 0.595\n",
      "epoch: 3     cost: 1.0700302332097829        accuracy: 0.635\n",
      "epoch: 4     cost: 0.9431145860932089        accuracy: 0.66\n",
      "epoch: 5     cost: 0.8733170550519772        accuracy: 0.705\n",
      "epoch: 6     cost: 0.8280347449129278        accuracy: 0.74\n",
      "epoch: 7     cost: 0.7427428644353695        accuracy: 0.765\n",
      "epoch: 8     cost: 0.7040681844407864        accuracy: 0.815\n",
      "epoch: 9     cost: 0.724781277179718        accuracy: 0.785\n",
      "epoch: 10     cost: 0.6842317314581435        accuracy: 0.745\n",
      "epoch: 11     cost: 0.6686345228281886        accuracy: 0.79\n",
      "epoch: 12     cost: 0.6306095294518902        accuracy: 0.8\n",
      "epoch: 13     cost: 0.6114167205853891        accuracy: 0.855\n",
      "epoch: 14     cost: 0.5924720854108984        accuracy: 0.835\n",
      "epoch: 15     cost: 0.5815529059280049        accuracy: 0.835\n",
      "epoch: 16     cost: 0.549929834495891        accuracy: 0.77\n",
      "epoch: 17     cost: 0.5332180077379398        accuracy: 0.84\n",
      "epoch: 18     cost: 0.5245059055631813        accuracy: 0.855\n",
      "epoch: 19     cost: 0.5349147143147207        accuracy: 0.825\n",
      "epoch: 20     cost: 0.5308308937332848        accuracy: 0.83\n",
      "epoch: 21     cost: 0.4999013182249936        accuracy: 0.84\n",
      "epoch: 22     cost: 0.5040912314978515        accuracy: 0.78\n",
      "epoch: 23     cost: 0.495452501448718        accuracy: 0.88\n",
      "epoch: 24     cost: 0.4870704023404558        accuracy: 0.81\n",
      "epoch: 25     cost: 0.4915328948064287        accuracy: 0.88\n",
      "epoch: 26     cost: 0.483461156433279        accuracy: 0.805\n",
      "epoch: 27     cost: 0.4412516770579601        accuracy: 0.895\n",
      "epoch: 28     cost: 0.4454211069237104        accuracy: 0.865\n",
      "epoch: 29     cost: 0.4356598074869676        accuracy: 0.86\n",
      "epoch: 30     cost: 0.4201400953531268        accuracy: 0.86\n",
      "훈련 종료\n",
      "정확도 :  0.8741\n"
     ]
    }
   ],
   "source": [
    "# 레이어 3개추가\n",
    "# 입출력 갯수: 256\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([28*28, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)\n",
    "\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.sigmoid(logit)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c, acc = sess.run([train, cost,accuracy], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost, \"       accuracy:\",acc)\n",
    "        \n",
    "print(\"훈련 종료\")\n",
    "\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 1.0977291003140535        accuracy: 0.875\n",
      "epoch: 2     cost: 0.3816365964304314        accuracy: 0.925\n",
      "epoch: 3     cost: 0.3112058398398488        accuracy: 0.955\n",
      "epoch: 4     cost: 0.2742936692996456        accuracy: 0.935\n",
      "epoch: 5     cost: 0.24580188832499777        accuracy: 0.95\n",
      "epoch: 6     cost: 0.22251864249056047        accuracy: 0.93\n",
      "epoch: 7     cost: 0.20290670630606747        accuracy: 0.96\n",
      "epoch: 8     cost: 0.18463401236317376        accuracy: 0.925\n",
      "epoch: 9     cost: 0.16973701645027497        accuracy: 0.955\n",
      "epoch: 10     cost: 0.15628738416866825        accuracy: 0.955\n",
      "epoch: 11     cost: 0.14310079799457037        accuracy: 0.965\n",
      "epoch: 12     cost: 0.13284415559335194        accuracy: 0.97\n",
      "epoch: 13     cost: 0.1232377056506546        accuracy: 0.955\n",
      "epoch: 14     cost: 0.11466087311506268        accuracy: 0.97\n",
      "epoch: 15     cost: 0.10670311797748912        accuracy: 0.97\n",
      "epoch: 16     cost: 0.10038206650452176        accuracy: 0.97\n",
      "epoch: 17     cost: 0.09435288566080002        accuracy: 0.965\n",
      "epoch: 18     cost: 0.08813496541570529        accuracy: 0.975\n",
      "epoch: 19     cost: 0.08259275006976996        accuracy: 0.955\n",
      "epoch: 20     cost: 0.07832020494748242        accuracy: 0.99\n",
      "epoch: 21     cost: 0.07316755487837576        accuracy: 0.975\n",
      "epoch: 22     cost: 0.06907297745347027        accuracy: 0.985\n",
      "epoch: 23     cost: 0.06505004380914296        accuracy: 0.985\n",
      "epoch: 24     cost: 0.06118465298278763        accuracy: 0.98\n",
      "epoch: 25     cost: 0.058284162834964        accuracy: 0.98\n",
      "epoch: 26     cost: 0.05473369277675044        accuracy: 0.995\n",
      "epoch: 27     cost: 0.05143852039832964        accuracy: 0.985\n",
      "epoch: 28     cost: 0.04858592880381779        accuracy: 0.985\n",
      "epoch: 29     cost: 0.04618371350690725        accuracy: 0.985\n",
      "epoch: 30     cost: 0.04321771811016582        accuracy: 0.99\n",
      "훈련 종료\n",
      "정확도 :  0.9744\n"
     ]
    }
   ],
   "source": [
    "# Xavier 초기화  \n",
    "# 그냥 Variable 이 아니라 get_variable로 해야한다.\n",
    "\n",
    "tf.reset_default_graph()   \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.get_variable('W1',shape=[28*28,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "# 기존처럼 랜덤으로 주는 값이 아닌 Xavier 초기화\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)\n",
    "\n",
    "\n",
    "W2 = tf.get_variable('W2',shape=[256,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "\n",
    "W3 = tf.get_variable('W3',shape=[256,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.sigmoid(logit)\n",
    "\n",
    "W4 = tf.get_variable('W4',shape=[256,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c, acc = sess.run([train, cost,accuracy], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost, \"       accuracy:\",acc)\n",
    "        \n",
    "print(\"훈련 종료\")\n",
    "\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 2.3708682701804418        accuracy: 0.095\n",
      "epoch: 2     cost: 2.3246289305253467        accuracy: 0.09\n",
      "epoch: 3     cost: 2.301135151603004        accuracy: 0.18\n",
      "epoch: 4     cost: 1.8878035528009596        accuracy: 0.475\n",
      "epoch: 5     cost: 1.2763256322253833        accuracy: 0.51\n",
      "epoch: 6     cost: 0.9857557914473801        accuracy: 0.8\n",
      "epoch: 7     cost: 0.6147457665746865        accuracy: 0.835\n",
      "epoch: 8     cost: 0.454976688515056        accuracy: 0.895\n",
      "epoch: 9     cost: 0.358595084006136        accuracy: 0.92\n",
      "epoch: 10     cost: 0.31568756732073683        accuracy: 0.965\n",
      "epoch: 11     cost: 0.22802324958822948        accuracy: 0.965\n",
      "epoch: 12     cost: 0.19122448075901377        accuracy: 0.97\n",
      "epoch: 13     cost: 0.18914419315078043        accuracy: 0.96\n",
      "epoch: 14     cost: 0.1400975351577456        accuracy: 0.97\n",
      "epoch: 15     cost: 0.12123781487345697        accuracy: 0.94\n",
      "epoch: 16     cost: 0.10932187946005309        accuracy: 0.97\n",
      "epoch: 17     cost: 0.09531271218576207        accuracy: 0.97\n",
      "epoch: 18     cost: 0.08747719841247259        accuracy: 0.995\n",
      "epoch: 19     cost: 0.07776510600339279        accuracy: 0.99\n",
      "epoch: 20     cost: 0.1419648316265507        accuracy: 0.975\n",
      "epoch: 21     cost: 0.06443805616010316        accuracy: 0.975\n",
      "epoch: 22     cost: 0.053180497146465555        accuracy: 0.995\n",
      "epoch: 23     cost: 0.04715040410784157        accuracy: 0.985\n",
      "epoch: 24     cost: 0.06885909960012547        accuracy: 0.975\n",
      "epoch: 25     cost: 0.0387724748931148        accuracy: 0.99\n",
      "epoch: 26     cost: 0.03277228276499292        accuracy: 0.975\n",
      "epoch: 27     cost: 0.027913274922492858        accuracy: 0.985\n",
      "epoch: 28     cost: 0.024539601950990902        accuracy: 1.0\n",
      "epoch: 29     cost: 0.02223561997271394        accuracy: 0.99\n",
      "epoch: 30     cost: 0.01939584972252223        accuracy: 0.995\n",
      "훈련 종료\n",
      "정확도 :  0.9724\n"
     ]
    }
   ],
   "source": [
    "# layer는 총 8개\n",
    "# 입출력 갯수는 512개\n",
    "\n",
    "tf.reset_default_graph()   \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.get_variable('W1',shape=[784,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "# 기존처럼 랜덤으로 주는 값이 아닌 Xavier 초기화\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)\n",
    "\n",
    "\n",
    "W2 = tf.get_variable('W2',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "\n",
    "W3 = tf.get_variable('W3',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 =  tf.nn.relu(logit)\n",
    "\n",
    "W4 = tf.get_variable('W4',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "layer4 =  tf.nn.relu(logit)\n",
    "\n",
    "W5 = tf.get_variable('W5',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer4, W5) + b5\n",
    "layer5 = tf.sigmoid(logit)\n",
    "\n",
    "\n",
    "W6 = tf.get_variable('W6',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer5, W6) + b6\n",
    "layer6 = tf.sigmoid(logit)\n",
    "\n",
    "\n",
    "W7 = tf.get_variable('W7',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer6, W7) + b7\n",
    "layer7 = tf.sigmoid(logit)\n",
    "\n",
    "W8 = tf.get_variable('W8',shape=[512,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer7, W8) + b8\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c, acc = sess.run([train, cost,accuracy], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost, \"       accuracy:\",acc)\n",
    "        \n",
    "print(\"훈련 종료\")\n",
    "\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-26-c3c77ac36040>:16: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "epoch: 1     cost: 2.4031455612182633        accuracy: 0.125\n",
      "epoch: 2     cost: 2.3365660329298548        accuracy: 0.155\n",
      "epoch: 3     cost: 2.3229872928966184        accuracy: 0.115\n",
      "epoch: 4     cost: 2.315997638702393        accuracy: 0.055\n",
      "epoch: 5     cost: 2.3130900669097905        accuracy: 0.105\n",
      "epoch: 6     cost: 2.310849611975929        accuracy: 0.025\n",
      "epoch: 7     cost: 2.3094823351773344        accuracy: 0.1\n",
      "epoch: 8     cost: 2.3076845767281258        accuracy: 0.085\n",
      "epoch: 9     cost: 2.306399550004439        accuracy: 0.085\n",
      "epoch: 10     cost: 2.3046599145369098        accuracy: 0.12\n",
      "epoch: 11     cost: 2.2978645905581403        accuracy: 0.125\n",
      "epoch: 12     cost: 2.200651138045572        accuracy: 0.135\n",
      "epoch: 13     cost: 1.9292659057270387        accuracy: 0.21\n",
      "epoch: 14     cost: 1.797371800596064        accuracy: 0.29\n",
      "epoch: 15     cost: 1.6648357746817848        accuracy: 0.28\n",
      "epoch: 16     cost: 1.5678666582974539        accuracy: 0.305\n",
      "epoch: 17     cost: 1.4646149232170809        accuracy: 0.375\n",
      "epoch: 18     cost: 1.362846031622452        accuracy: 0.355\n",
      "epoch: 19     cost: 1.2701878565007994        accuracy: 0.39\n",
      "epoch: 20     cost: 1.2004762155359447        accuracy: 0.43\n",
      "epoch: 21     cost: 1.1318146142092622        accuracy: 0.41\n",
      "epoch: 22     cost: 1.0261580499735747        accuracy: 0.53\n",
      "epoch: 23     cost: 0.7849493102593859        accuracy: 0.62\n",
      "epoch: 24     cost: 0.596437107974833        accuracy: 0.66\n",
      "epoch: 25     cost: 0.5055919750170276        accuracy: 0.6\n",
      "epoch: 26     cost: 0.4420115430246701        accuracy: 0.665\n",
      "epoch: 27     cost: 0.3950037716193634        accuracy: 0.6\n",
      "epoch: 28     cost: 0.361398325779221        accuracy: 0.685\n",
      "epoch: 29     cost: 0.3310283700444481        accuracy: 0.64\n",
      "epoch: 30     cost: 0.30301287055015563        accuracy: 0.65\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "#Drop Out\n",
    "\n",
    "tf.reset_default_graph()   \n",
    "\n",
    "prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.get_variable('W1',shape=[784,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "# 기존처럼 랜덤으로 주는 값이 아닌 Xavier 초기화\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)\n",
    "layer1 = tf.nn.dropout(layer1,keep_prob=prob)\n",
    "\n",
    "\n",
    "\n",
    "W2 = tf.get_variable('W2',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "layer2 = tf.nn.dropout(layer2,keep_prob=prob)\n",
    "\n",
    "\n",
    "W3 = tf.get_variable('W3',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 =  tf.nn.relu(logit)\n",
    "layer3 = tf.nn.dropout(layer3,keep_prob=prob)\n",
    "\n",
    "\n",
    "\n",
    "W4 = tf.get_variable('W4',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "layer4 =  tf.nn.relu(logit)\n",
    "layer4 = tf.nn.dropout(layer4,keep_prob=prob)\n",
    "\n",
    "\n",
    "W5 = tf.get_variable('W5',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer4, W5) + b5\n",
    "layer5 = tf.sigmoid(logit)\n",
    "layer5 = tf.nn.dropout(layer5,keep_prob=prob)\n",
    "\n",
    "\n",
    "W6 = tf.get_variable('W6',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer5, W6) + b6\n",
    "layer6 = tf.sigmoid(logit)\n",
    "layer6 = tf.nn.dropout(layer6,keep_prob=prob)\n",
    "\n",
    "W7 = tf.get_variable('W7',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer6, W7) + b7\n",
    "layer7 = tf.sigmoid(logit)\n",
    "layer7 = tf.nn.dropout(layer7,keep_prob=prob)\n",
    "\n",
    "\n",
    "W8 = tf.get_variable('W8',shape=[512,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer7, W8) + b8\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "hypothesis = tf.nn.dropout(hypothesis,keep_prob=prob)\n",
    "\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c, acc = sess.run([train, cost,accuracy], feed_dict={X:batch_xs, y:batch_ys,prob:0.7})   \n",
    "                                                   #prob 0.7   ->  70% 만 가지고 실행\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost, \"       accuracy:\",acc)\n",
    "        \n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.9492\n"
     ]
    }
   ],
   "source": [
    "correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accu = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "print(\"정확도 : \", sess.run(accu, feed_dict={X:mnist.test.images, y:mnist.test.labels,prob:1}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 1.180615014162931        accuracy: 0.62\n",
      "epoch: 2     cost: 0.29590179595080274        accuracy: 0.685\n",
      "epoch: 3     cost: 0.18618140865455973        accuracy: 0.695\n",
      "epoch: 4     cost: 0.14263820004734132        accuracy: 0.71\n",
      "epoch: 5     cost: 0.11932225148786184        accuracy: 0.66\n",
      "epoch: 6     cost: 0.1053581786291166        accuracy: 0.655\n",
      "epoch: 7     cost: 0.09266472137109795        accuracy: 0.68\n",
      "epoch: 8     cost: 0.08462306354533539        accuracy: 0.72\n",
      "epoch: 9     cost: 0.07547722630202772        accuracy: 0.73\n",
      "epoch: 10     cost: 0.07076663807034489        accuracy: 0.695\n",
      "epoch: 11     cost: 0.06463094206357545        accuracy: 0.71\n",
      "epoch: 12     cost: 0.061218106424943984        accuracy: 0.71\n",
      "epoch: 13     cost: 0.05718061084774406        accuracy: 0.66\n",
      "epoch: 14     cost: 0.05008767264133151        accuracy: 0.705\n",
      "epoch: 15     cost: 0.05187079580839381        accuracy: 0.69\n",
      "epoch: 16     cost: 0.04454846692356197        accuracy: 0.615\n",
      "epoch: 17     cost: 0.043636630898849504        accuracy: 0.705\n",
      "epoch: 18     cost: 0.04603766533038155        accuracy: 0.65\n",
      "epoch: 19     cost: 0.04379760427569802        accuracy: 0.63\n",
      "epoch: 20     cost: 0.039006180773404495        accuracy: 0.69\n",
      "epoch: 21     cost: 0.03901269050569019        accuracy: 0.705\n",
      "epoch: 22     cost: 0.040103986176916115        accuracy: 0.625\n",
      "epoch: 23     cost: 0.03893138520165601        accuracy: 0.735\n",
      "epoch: 24     cost: 0.034588007306341456        accuracy: 0.69\n",
      "epoch: 25     cost: 0.03292522309813646        accuracy: 0.7\n",
      "epoch: 26     cost: 0.03403473839841105        accuracy: 0.685\n",
      "epoch: 27     cost: 0.03487734613432127        accuracy: 0.72\n",
      "epoch: 28     cost: 0.03386619431390004        accuracy: 0.695\n",
      "epoch: 29     cost: 0.030033306136558007        accuracy: 0.7\n",
      "epoch: 30     cost: 0.03193779160395603        accuracy: 0.645\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "#Drop Out\n",
    "\n",
    "tf.reset_default_graph()   \n",
    "\n",
    "prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.get_variable('W1',shape=[784,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "# 기존처럼 랜덤으로 주는 값이 아닌 Xavier 초기화\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit)\n",
    "layer1 = tf.nn.dropout(layer1,keep_prob=prob)\n",
    "\n",
    "\n",
    "\n",
    "W2 = tf.get_variable('W2',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit)\n",
    "layer2 = tf.nn.dropout(layer2,keep_prob=prob)\n",
    "\n",
    "\n",
    "W3 = tf.get_variable('W3',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "layer3 =  tf.nn.relu(logit)\n",
    "layer3 = tf.nn.dropout(layer3,keep_prob=prob)\n",
    "\n",
    "\n",
    "\n",
    "W4 = tf.get_variable('W4',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer3, W4) + b4\n",
    "layer4 =  tf.nn.relu(logit)\n",
    "layer4 = tf.nn.dropout(layer4,keep_prob=prob)\n",
    "\n",
    "\n",
    "W5 = tf.get_variable('W5',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer4, W5) + b5\n",
    "layer5 = tf.sigmoid(logit)\n",
    "layer5 = tf.nn.dropout(layer5,keep_prob=prob)\n",
    "\n",
    "\n",
    "W6 = tf.get_variable('W6',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer5, W6) + b6\n",
    "layer6 = tf.sigmoid(logit)\n",
    "layer6 = tf.nn.dropout(layer6,keep_prob=prob)\n",
    "\n",
    "W7 = tf.get_variable('W7',shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit = tf.matmul(layer6, W7) + b7\n",
    "layer7 = tf.sigmoid(logit)\n",
    "layer7 = tf.nn.dropout(layer7,keep_prob=prob)\n",
    "\n",
    "\n",
    "W8 = tf.get_variable('W8',shape=[512,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer7, W8) + b8\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "hypothesis = tf.nn.dropout(hypothesis,keep_prob=prob)\n",
    "\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "is_corrected = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corrected, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c, acc = sess.run([train, cost,accuracy], feed_dict={X:batch_xs, y:batch_ys,prob:0.7})   \n",
    "                                                   #prob 0.7   ->  70% 만 가지고 실행\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost, \"       accuracy:\",acc)\n",
    "        \n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
